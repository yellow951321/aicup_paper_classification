{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efb710a3910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Task 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D00002</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>Sadeghi/Smith</td>\n",
       "      <td>cs.SY/cs.RO/math.OC</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D00003</td>\n",
       "      <td>Data-driven Upsampling of Point Clouds</td>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>Zhang/Jiang/Yang/Yamakawa/Shimada/Kara</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2018-07-07</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D00004</td>\n",
       "      <td>Accessibility or Usability of InteractSE? A He...</td>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>Aqle/Khowaja/Al-Thani</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D00005</td>\n",
       "      <td>Spatio-Temporal Facial Expression Recognition ...</td>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>Hasani/Mahoor</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              Title  \\\n",
       "0  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "1  D00002  On Efficient Computation of Shortest Dubins Pa...   \n",
       "2  D00003             Data-driven Upsampling of Point Clouds   \n",
       "3  D00004  Accessibility or Usability of InteractSE? A He...   \n",
       "4  D00005  Spatio-Temporal Facial Expression Recognition ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Rapid popularity of Internet of Things (IoT) a...   \n",
       "1  In this paper, we address the problem of compu...   \n",
       "2  High quality upsampling of sparse 3D point clo...   \n",
       "3  Internet is the main source of information now...   \n",
       "4  Automated Facial Expression Recognition (FER) ...   \n",
       "\n",
       "                                             Authors            Categories  \\\n",
       "0  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "1                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n",
       "2             Zhang/Jiang/Yang/Yamakawa/Shimada/Kara                 cs.CV   \n",
       "3                              Aqle/Khowaja/Al-Thani                 cs.HC   \n",
       "4                                      Hasani/Mahoor                 cs.CV   \n",
       "\n",
       "  Created Date       Task 2  \n",
       "0   2018-01-11  THEORETICAL  \n",
       "1   2016-09-21  THEORETICAL  \n",
       "2   2018-07-07  ENGINEERING  \n",
       "3   2018-08-29    EMPIRICAL  \n",
       "4   2017-03-20  ENGINEERING  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/task2_trainset.csv', dtype=str)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv('data/trainset.csv', index=False)\n",
    "validset.to_csv('data/validset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_words(data_path, n_workers=4):\n",
    "    df = pd.read_csv(data_path, dtype=str)\n",
    "        \n",
    "    sent_list = []\n",
    "    for i in df.iterrows():\n",
    "        sent_list += i[1]['Abstract'].split('$$$')\n",
    "\n",
    "    chunks = [\n",
    "        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n",
    "        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n",
    "    ]\n",
    "    with Pool(n_workers) as pool:\n",
    "        chunks = pool.map_async(word_tokenize, chunks)\n",
    "        words = set(sum(chunks.get(), []))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "words |= collect_words('data/trainset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n",
    "for word in words:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        embedding_path (str): Path where embedding are loaded from (text file).\n",
    "        words (None or list): If not None, only load embedding of the words in\n",
    "            the list.\n",
    "        oov_as_unk (bool): If argument `words` are provided, whether or not\n",
    "            treat words in `words` but not in embedding file as `<unk>`. If\n",
    "            true, OOV will be mapped to the index of `<unk>`. Otherwise,\n",
    "            embedding of those OOV will be randomly initialize and their\n",
    "            indices will be after non-OOV.\n",
    "        lower (bool): Whether or not lower the words.\n",
    "        rand_seed (int): Random seed for embedding initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_path, words=None, oov_as_unk=True, lower=True, rand_seed=524):\n",
    "        self.word_dict = {}\n",
    "        self.vectors = None\n",
    "        self.lower = lower\n",
    "        self.extend(embedding_path, words, oov_as_unk)\n",
    "        torch.manual_seed(rand_seed)\n",
    "\n",
    "        if '<pad>' not in self.word_dict:\n",
    "            self.add(\n",
    "                '<pad>', torch.zeros(self.get_dim())\n",
    "            )\n",
    "        \n",
    "        if '<bos>' not in self.word_dict:\n",
    "            t_tensor = torch.rand((1, self.get_dim()), dtype=torch.float)\n",
    "            torch.nn.init.orthogonal_(t_tensor)\n",
    "            self.add(\n",
    "                '<bos>', t_tensor\n",
    "            )\n",
    "            \n",
    "        if '<eos>' not in self.word_dict:\n",
    "            t_tensor = torch.rand((1, self.get_dim()), dtype=torch.float)\n",
    "            torch.nn.init.orthogonal_(t_tensor)\n",
    "            self.add(\n",
    "                '<eos>', t_tensor\n",
    "            )\n",
    "        \n",
    "        if '<unk>' not in self.word_dict:\n",
    "            self.add('<unk>')\n",
    "\n",
    "    def to_index(self, word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word (str)\n",
    "\n",
    "        Return:\n",
    "             index of the word. If the word is not in `words` and not in the\n",
    "             embedding file, then index of `<unk>` will be returned.\n",
    "        \"\"\"\n",
    "        if self.lower:\n",
    "            word = word.lower()\n",
    "\n",
    "        if word not in self.word_dict:\n",
    "            return self.word_dict['<unk>']\n",
    "        else:\n",
    "            return self.word_dict[word]\n",
    "\n",
    "    def get_dim(self):\n",
    "        return self.vectors.shape[1]\n",
    "\n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vectors.shape[0]\n",
    "\n",
    "    def add(self, word, vector=None):\n",
    "        if self.lower:\n",
    "            word = word.lower()\n",
    "\n",
    "        if vector is not None:\n",
    "            vector = vector.view(1, -1)\n",
    "        else:\n",
    "            vector = torch.empty(1, self.get_dim())\n",
    "            torch.nn.init.uniform_(vector)\n",
    "        self.vectors = torch.cat([self.vectors, vector], 0)\n",
    "        self.word_dict[word] = len(self.word_dict)\n",
    "\n",
    "    def extend(self, embedding_path, words, oov_as_unk=True):\n",
    "        self._load_embedding(embedding_path, words)\n",
    "\n",
    "        if words is not None and not oov_as_unk:\n",
    "            # initialize word vector for OOV\n",
    "            for word in words:\n",
    "                if self.lower:\n",
    "                    word = word.lower()\n",
    "\n",
    "                if word not in self.word_dict:\n",
    "                    self.word_dict[word] = len(self.word_dict)\n",
    "\n",
    "            oov_vectors = torch.nn.init.uniform_(\n",
    "                torch.empty(len(self.word_dict) - self.vectors.shape[0],\n",
    "                            self.vectors.shape[1]))\n",
    "\n",
    "            self.vectors = torch.cat([self.vectors, oov_vectors], 0)\n",
    "\n",
    "    def _load_embedding(self, embedding_path, words):\n",
    "        if words is not None:\n",
    "            words = set(words)\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        with open(embedding_path) as fp:\n",
    "\n",
    "            row1 = fp.readline()\n",
    "            # if the first row is not header\n",
    "            if not re.match('^[0-9]+ [0-9]+$', row1):\n",
    "                # seek to 0\n",
    "                fp.seek(0)\n",
    "            # otherwise ignore the header\n",
    "\n",
    "            for i, line in enumerate(fp):\n",
    "                cols = line.rstrip().split(' ')\n",
    "                word = cols[0]\n",
    "\n",
    "                # skip word not in words if words are provided\n",
    "                if words is not None and word not in words:\n",
    "                    continue\n",
    "                elif word not in self.word_dict:\n",
    "                    self.word_dict[word] = len(self.word_dict)\n",
    "                    vectors.append([float(v) for v in cols[1:]])\n",
    "\n",
    "        vectors = torch.tensor(vectors)\n",
    "        if self.vectors is not None:\n",
    "            self.vectors = torch.cat([self.vectors, vectors], dim=0)\n",
    "        else:\n",
    "            self.vectors = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedding('./tools/glove.840B.300d.txt', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0828,  0.6720, -0.1499,  ..., -0.1918, -0.3785, -0.0659],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        [ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n",
       "        ...,\n",
       "        [ 0.0185,  0.0574, -0.0387,  ...,  0.0095, -0.0445,  0.0636],\n",
       "        [ 0.0560, -0.0576, -0.0657,  ..., -0.0406, -0.0303, -0.0253],\n",
       "        [ 0.0463,  0.9254,  0.3792,  ...,  0.5435,  0.9610,  0.9441]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding.pkl','wb') as f:\n",
    "    pickle.dump(embedder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n",
    "    onehot = [0,0,0,0]\n",
    "    for l in labels.split():\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sentence, word_dict):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [word_dict.to_index(word) for word in word_tokenize(sentence)]\n",
    "    \n",
    "def get_dataset(data_path, word_dict, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, word_dict):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], word_dict))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, word_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n",
    "    if 'Task 2' in data:\n",
    "        processed['Label'] = label_to_onehot(data['Task 2'])\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start processing trainset...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset('./data/trainset.csv', embedder, n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start processing validset...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset('./data/validset.csv', embedder, n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractDataset(Dataset):\n",
    "    def __init__(self, data, pad_idx, max_len = 300):\n",
    "        self.data = data\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_sent = max([len(data['Abstract']) for data in datas])\n",
    "        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n",
    "        batch_abstract = []\n",
    "        batch_label = []\n",
    "        sent_len = []\n",
    "        for data in datas:\n",
    "            # padding abstract to make them in same length\n",
    "            pad_abstract = []\n",
    "            for sentence in data['Abstract']:\n",
    "                if len(sentence) > max_len:\n",
    "                    pad_abstract.append(sentence[:max_len])\n",
    "                else:\n",
    "                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n",
    "            sent_len.append(len(pad_abstract))\n",
    "            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n",
    "            batch_abstract.append(pad_abstract)\n",
    "            \n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                batch_label.append(data['Label'])\n",
    "                \n",
    "        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = AbstractDataset(train, PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validData = AbstractDataset(valid, PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.hidden_dim = 512\n",
    "        self.sent_rnn = nn.GRU(vocabulary_size,\n",
    "                                self.hidden_dim,\n",
    "                                bidirectional=True,\n",
    "                                batch_first=True)\n",
    "        self.l1 = nn.Linear(self.hidden_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,s,w,e = x.shape\n",
    "        x = x.view(b,s*w,e)\n",
    "        x, __ = self.sent_rnn(x)\n",
    "        x = x.view(b,s,w,-1)\n",
    "        x = torch.max(x,dim=2)[0]\n",
    "        x = x[:,:,:self.hidden_dim] + x[:,:,self.hidden_dim:]\n",
    "        x = torch.max(x,dim=1)[0]\n",
    "        x = torch.sigmoid(self.l1(F.relu(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.5\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts = predicts > self.threshold\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.uint8) * predicts).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20)\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_epoch(epoch, embedding, training):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        description = 'Train'\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=2)\n",
    "\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0\n",
    "    f1_score = F1()\n",
    "    for i, (x, y, sent_len) in trange:\n",
    "        x = embedding(x)\n",
    "        o_labels, batch_loss = _run_iter(x,y)\n",
    "        if training:\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        f1_score.update(o_labels.cpu(), y)\n",
    "\n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "    if training:\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "    else:\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "    gc.collect();\n",
    "def _run_iter(x,y):\n",
    "    abstract = x.to(device)\n",
    "    labels = y.to(device)\n",
    "    o_labels = model(abstract)\n",
    "    l_loss = criteria(o_labels, labels)\n",
    "    return o_labels, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model.pkl.'+str(epoch))\n",
    "    with open('model/history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleNet(embedder.get_dim())\n",
    "opt = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "criteria = torch.nn.BCELoss()\n",
    "model.to(device)\n",
    "max_epoch = 25\n",
    "history = {'train':[],'valid':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(embedder.get_vocabulary_size(),embedder.get_dim())\n",
    "embedding.weight = torch.nn.Parameter(embedder.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    _run_epoch(epoch, embedding, True)\n",
    "    _run_epoch(epoch, embedding, False)\n",
    "    save(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['train']\n",
    "history['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('outfile1', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start processing testset...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/task2_public_testset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset('testset.csv', embedder, n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = [AbstractDataset(test[2000*n:2000*(n+1)], PAD_TOKEN) for n in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = testData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce2586a243640869f327740b38ae613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=22, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ed5ea8afc646fcb472b319efafe175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predict', max=16, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model/model.pkl.{}'.format(7)))\n",
    "model.train(False)\n",
    "_run_epoch(1, embedding, False)\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=128,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=0)\n",
    "trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "prediction = []\n",
    "for i, (x,y,_l) in trange:\n",
    "    x = embedding(x)\n",
    "    o_labels = model(x.to(device))\n",
    "    o_labels = o_labels>0.5\n",
    "    prediction.append(o_labels.to('cpu'))\n",
    "    gc.collect()\n",
    "\n",
    "prediction = torch.cat(prediction).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction (numpy array)\n",
    "        sampleFile (str)\n",
    "        public (boolean)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n",
    "    else:\n",
    "        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outfile2', 'wb') as fp:\n",
    "    pickle.dump(prediction, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubmitGenerator(prediction, \n",
    "                'data/task2_sample_submission.csv',\n",
    "                True, \n",
    "                './task2_submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
